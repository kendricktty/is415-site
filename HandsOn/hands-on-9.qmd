---
title: "HX9/10: Geographically Weighted Regression - Calibrating a Hedonic Pricing Model"
subtitle: "Hands-On Exercise 9 & 10"
author: "Kendrick Teo"
date: "2024-10-20"
date-modified: "last-modified"

execute: 
  eval: true
  echo: true
  freeze: true
---

## HX9/10.1 Overview, Data and R Packages

**Regression** is a statistical (and machine learning) technique that uses non-stationary variables to model the relationship between these independent variables and a **numerical** outcome of interest (dependent variable), the aim of which is to *predict* unseen values. **Geographically weighted regression** factors in *location* variables like climate and physical environment characteristics. In this exercise, we will build a hedonic pricing model to predict the resale prices of condominiums in [Singapore](https://en.wikipedia.org/wiki/Singapore) in 2015.

The datasets to be used in this exercise are:

-   URA Master Plan Subzone Boundary in Shapefile format (i.e. `MP14_SUBZONE_WEB_PL`), and
-   `condo_resale_2015.csv`.

The R packages to be used today are:

-   `olsrr` for building OLS and performing diagnostic tests
-   `GWmodel`, for building geographically weighted models
-   `corrplot` for data visualisation and analysis
-   `sf`, `tidyverse` and `tmap` for data handling and choropleth mapping.

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)
```

### HX9/10.1.1 A short note about `GWmodel`

The `GWmodel` package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression. Some of these are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the `GWmodel` are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis.

## HX9/10.2 Geospatial Data Wrangling

### HX9/10.2.1 Importing data

```{r}
mpsz <- st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

### HX9/10.2.2 Update CRS information

The coordinate system needs to be updated to Singapore's: ESPG `3414`, or SVY21.

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
st_crs(mpsz_svy21)
st_bbox(mpsz_svy21) #view extent
```

## HX9/10.3 Aspatial Data Wrangling

### HX9/10.3.1 Importing data

```{r}
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")
glimpse(condo_resale)
```

### HX9/10.3.2 Converting aspatial dataframe to `sf` object

As the `condo_resale` dataframe is aspatial, we will now need to convert it to an `sf` object. Notice we need to perform another `st_transform()` to convert the coordinates from WGS84 to SVY21.

```{r}
condo_resale.sf <- st_as_sf(condo_resale, coords = c("LONGITUDE", "LATITUDE"), crs=4326) %>% st_transform(crs=3414)
head(condo_resale.sf)
```

## HX9/10.4 Exploratory Data Analysis

**Exploratory data analysis (EDA)** is a crucial part of any data analytics task, and is especially critical for data preparation prior to machine learning. In this section, we will use `ggplot2` to perform EDA.

### HX9/10.4.1 Using graphical methods

The distribution of `SELLING_PRICE` may be plotted using the following code chunk.

```{r}
ggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="green4")
```

The distribution of `SELLING_PRICE` is **right skewed**, meaning that more condominium units were transacted at lower prices.

We can normalise this data using log transformation. The code chunk below is used to derive a new variable called `LOG_SELLING_PRICE` by using a log transformation on the variable \`SELLING_PRICE, using mutate() of dplyr package. The end result is a distribution that is less skewed.

```{r}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
ggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="green4")
```

## HX9/10.4.2 Multiple histogram plots

`ggarrange()` of the `ggpubr` package may be used to draw multiple small histograms, achieving a similar capability found in `matplotlib` of Python. In the code chunk below, 12 histograms are created and then presented as a table of 3 columns and 4 rows.

```{r}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="green4")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="green4")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="green4")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="green4")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="green4")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="green4")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="green4")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="green4")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="green4")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="green4")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="green4")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="green4")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

## HX9/10.5 Geospatial EDA

### HX9/10.5.1 Statistical point map

Finally, we want to draw the geospatial distribution of condo resale prices in Singapore with the `tmap` package.

First, we will turn on the interactive mapping mode with the code chunk below.

```{r}
tmap_options(check.and.fix = TRUE)
tmap_mode("view")
```

Next, the code chunks below will draw us an interactive point symbol map.

```{r}
tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

Notice that `tm_dots()` is used instead of `tm_bubbles()`. Also, the `set.zoom.limits` argument of `tm_view()` sets the minimum and maximum zoom level to `11` and `14` respectively.

Before moving on to the next section, it is a good practice to revert `tmap` back to `plot` mode.

```{r}
tmap_mode("plot")
```

## HX9/10.6 Hedonic Price Modelling in R

And now for the fun part: building our hedonic pricing models. We can do this using the **simple linear regression** or **multiple linear regression** methods.

### HX9/10.6.1 Simple Linear Regression

For this model, we want to predict `SELLING_PRICE` (`y`) using the independent variable `AREA_SQM` (`x`). In other words, we want to create a best fit linear function measuring the effect of `x` on `y`.

```{r}
condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)
```

The functions `summary()` and `anova()` can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by `lm`. `lm()` returns an object of class `lm`, or a vector `c("mlm", "lm")` for multiple responses.

```{r}
summary(condo.slr)
```

The function given to us is:

$y=14719x_1 -258121.1$

The coefficient of determination $R^2$ is $0.4518$. This indicates the model is able to explain about $45%$ of the resale prices. With a p-value of much smaller than `0.0001`, we can reject the null hypothesis that the mean is a good esimator of `SELLING_PRICE`. This will allow us to infer that our simple linear regression model is a good estimator of `SELLING_PRICE`.

We can visualise our best fit on a scatterplot by incorporating `lm()` as a function in `ggplot`'s geometry as shown below.

```{r}
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

A few outliers with relatively higher selling prices can be seen in the plot.

### HX9/10.6.2 Multiple Linear Regression

#### HX9/10.6.2.1 Additional EDA: Correlation heatmap

So far, we have only been using a single independent variable to predict housing prices. We can also use multiple independent variables to do likewise.

Before building our model, we need to ensure that the independent variables used are not highly correlated with one another, or the quality of the model will be compromised. In statistics, this phenomenon is known as **multicolinearity**. The typical method of doing this is to use a correlation matrix drawn as a heatmap to easily visualise the correlation coefficients of any given pair of variables. In Python, we use `pandas.DataFrame.corr()` to create the matrix and `seaborn.heatmap()` to visualise it as a heatmap. In R, the `corrplot` package is used instead.

```{r}
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

With a correlation coefficient of `-0.84`, `FREEHOLD` is highly correlated to `LEASE_99YEAR`. This could be because the two are mutually exclusive - a 99-year leasehold property must be returned to the government after 99 years. In view of this, it is wiser to only include either one of them in the subsequent model building. We will therefore exclude `LEASE_99YEAR` in the subsequent model building.

#### HX9/10.6.2.2 Building hedonic pricing model using multiple linear regression

`lm()` can also be used to calibrate a multiple linear regression model.

```{r}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale.sf)
summary(condo.mlr)
```

### HX9/10.6.3 Preparing Publication Quality Table

#### HX9/10.6.3.1 `olsrr` method

From the `p`-values in the report above, some variables, such as `PROX_KINDERGARTEN` and `PROX_TOP_PRIMARY_SCH`, are not considered statistically significant at a 95% confidence interval. We can revise the model by removing these values.

Now, we are ready to calibrate the revised model with the code chunk below.

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)
ols_regress(condo.mlr1)
```

#### HX9/10.6.3.2 `gtsummary` method

A more elegant and flexible way to draw a publication-ready summary table is using `tbl_regression()` from the `gtsummary` package.

```{r}
tbl_regression(condo.mlr1, intercept = TRUE)
```

Model statistics may be included in the report either by appending them to the report table with `add_glance_table()` or by adding a source note with `add_glance_source_note()` as below.

```{r}
tbl_regression(condo.mlr1, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

## HX9/10.7 OLS Regression

OLS regression can be performed with the `oslrr` package. It provides a collection of useful methods for building multiple linear regression models, such as:

-   comprehensive regression output
-   residual diagnostics
-   measures of influence
-   heteroskedasticity tests
-   collinearity diagnostics
-   model fit assessment
-   variable contribution assessment
-   variable selection procedures

### HX9/10.7.1 Check for multicolinearity

In the code chunk below, `ols_vif_tol()` is used to test for multicolinearity.

```{r}
ols_vif_tol(condo.mlr1)
```

Since the VIFs of all our independent variables are less than 10, we can safely conclude that there is no sign of multicolinearity among our independent variables.

### HX9/10.7.2 Test for non-linearity

In multiple linear regression, it is important to test the assumption of linearity and additivity in the relationship between our dependent and independent variables. In the code chunk below, `ols_plot_resid_fit()` is used to perform the linearity assumption test.

```{r}
ols_plot_resid_fit(condo.mlr1)
```

Since most of our data points are scattered around the `0` line, we can conclude that there exists a linear relationship between the dependent variable and independent variables.

### HX9/10.7.3 Test for normality

`ols_plot_resid_hist()` is used to test for normality.

```{r}
ols_plot_resid_hist(condo.mlr1)
```

The plot reveals that the residual of the multiple linear regression model (`condo.mlr1`) resembles normal distribution.

Another way to test for normality is with statistical tests, which can be called with `ols_test_normality()` below.

```{r}
ols_test_normality(condo.mlr1)
```

From the summary table above, assuming a 95% confidence interval, the low p-values means we can infer that there is statistical evidence that the residuals are not normally distributed.

### HX9/10.7.4 Test for spatial autocorrelation

The hedonic model we are trying to build uses geographically referenced attributes. Hence, it is also important to test the residuals of our hedonic pricing model for spatial autocorrelation. The first step in doing so is to bind our residuals to our original dataset for visualisation by means of `tmap`.

```{r}
mlr.output <- as.data.frame(condo.mlr1$residuals)
condo_resale.res.sf <- cbind(condo_resale.sf, condo.mlr1$residuals) %>%
  rename(`MLR_RES` = `condo.mlr1.residuals`)
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

```{r}
# Display with tmap
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

The map above reveals there are signs of spatial autocorrelation. To verify that our observation is indeed true, we can perform the **[Moran's I](HandsOn/hands-on-6.qmd)** test.

```{r}
# Compute distance-based weight matrix
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
# Create spatial weights
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

We can use `lm.morantest()` from the `spdep` package to perform the Moran's I test for residual spatial autocorrelation.
```{r}
lm.morantest(condo.mlr1, nb_lw)
```

With a p-value of `2.2e-16`, we can reject the null hypothesis that the residuals are randomly distributed, for a 95% confidence interval. Since the observed Global Moran's I is `0.142 > 0`, we can infer that the residuals resemble cluster distribution.

## HX9/10.8 `GWmodel` Hedonic Pricing Model

### HX9/10.8.1 Computing fixed bandwidth

`bw.gr()` from GWmodel is used to determine the optimal fixed bandwidth.

`adaptive = FALSE` indicates we wish to compute fixed bandwidth. The `approach` argument determines whether we wish to use the CV cross-validation approach or the AIC corrected approach.

```{r}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                     FAMILY_FRIENDLY + FREEHOLD, 
                   data=condo_resale.sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)
```

The recommended bandwidth is 971.3405 metres - a result of using the SWY21 coordinate model which uses metres - close to 1km.

### HX9/10.8.2 Building `GWmodel` with fixed bandwidth

We can now build our `gwr` model using fixed bandwidth and the `gaussian` kernel. The output is saved as a `gwrm` object.

```{r}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                         FAMILY_FRIENDLY + FREEHOLD, 
                       data=condo_resale.sp, 
                       bw=bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
gwr.fixed
```

The AICc of the GWR is `42263.61`, which is significantly smaller than the `42967.1` derived with global regression.

### HX9/10.8.3 Building `GWmodel` with adaptive bandwidth

We can also calibrate a `gwr`-based hedonic pricing model using the adaptive bandwidth approach. The steps are similar to that for the fixed bandwidth approach, except this time we change the `adaptive` argument to `TRUE`.

```{r}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + 
                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + 
                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + 
                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data=condo_resale.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data=condo_resale.sp, bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)
gwr.adaptive
```

The AICc of the adaptive distance `gwr` is `41982.22`, which is smaller than the `42263.61` derived with our fixed distance `gwr`.

### HX9/10.8.4 Visualising GWR output

In addition to regression residuals, the output feature class table includes the:

- Condition Number `cond`: This diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than $30$ may be unreliable.

- Local $R^2$: Ranging between 0.0 and 1.0, local $R^2$ indicates how well the local regression model fits observed $y$ values. Very low values indicate the local model is performing poorly. Mapping the local $R^2$ values may provide clues about important variables that may be missing from the regression model, as it allows us to pinpoint locations where the model predicts well, and locations where it predicts poorly.

- Predicted: these are the estimated (or fitted) $y$ values computed by GWR.

- Residuals: To obtain the residual values, the fitted $y$ values, $\hat{y}$, are subtracted from the observed $y$ values. Standardised residuals have a mean of $0$ and a standard deviation of $1$. A cold-to-hot rendered map of standardised residuals can be produced using these values.

- Coefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in these estimates is higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.

These are stored in a `SpatialPointsDataFrame` or `SpatialPolygonsDataFrame` object integrated with `fit.points`, GWR coefficient estimates, y value, predicted value, coefficient standard errors and t-values in its `data slot` in an object called `SDF` in the output list.

The following sections serve to map these values.

### HX9/10.8.5 Converting SDF into `sf`

First, we need to convert `SDF` to an `sf` data frame. Subsequently, we can use `glimpse()` to display its content.

```{r}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)
condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)
condo_resale.sf.adaptive.svy21  
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))
glimpse(condo_resale.sf.adaptive)
summary(gwr.adaptive$SDF$yhat)
```

### HX9/10.8.6 Visualising local R^2

```{r}
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
```

### HX9/10.8.7 Visualising coefficient estimates

```{r}
AREA_SQM_SE <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

AREA_SQM_TV <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=2,
             sync = TRUE)
```

### HX9/10.8.8 Visualising individual URA planning Regions

```{r}
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
tm_shape(condo_resale.sf.adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)
```

## References

1.  Kam, T. S. (2024). 13 Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method. R for Geospatial Data Science and Analytics. <https://r4gdsa.netlify.app/chap13.html#hedonic-pricing-modelling-in-r>
