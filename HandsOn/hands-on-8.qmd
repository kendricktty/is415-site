---
title: "HX8: Spatially Constrained Clustering Analysis"
subtitle: "Hands-On Exercise 8"
author: "Kendrick Teo"
date: "2024-10-10"
date-modified: "last-modified"

execute: 
  eval: true
  echo: true
  freeze: true
---

## HX8.1 Overview

This hands-on exercise is a continuation of [Hands-on Exercise 7](hands-on-7.qmd), where the focus now will be on **spatially constrained clustering analysis**.

## HX8.2 Loading R packages and preparing data

In this section, we will load the R packages and the data we need, which are the same as Hands-on Exercise 7. We will also replicate the steps from the earlier exercise to derive the penetration rate of various ICT device types.

```{r}
pacman::p_load(spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
# Retrieve joined DF as saved in exercise 7
shan_sf <- read_rds("data/rds/shan_sf.rds")
ict <- read_csv ("data/aspatial/Shan-ICT.csv")

ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*100) %>%
  mutate(`TV_PR` = `Television`/`Total households`*100) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*100) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*100) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*100) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*100) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`)
summary(ict_derived)
```

### HX8.2.1 Abridged EDA

As a recap, we will also draw the choropleth maps and correlation matrix from the last exercise.

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

## HX8.3 Spatially Constrained Clustering: SKATER approach

This section serves to derive spatially constrained clusters using the `skater()` function from the `spdep` package. The **SKATER** (**S**patial '**K**'luster **A**nalysis by **T**ree **E**dge **R**emoval) method builds off a connectivity *graph* representing spatial relationships between neighbouring areas. In this graph, each area is represented by a *node* and each *edge* represents a connection between areas. The dissimilarity between neighbouring areas constitutes the *edge costs*, and we reduce the graph by pruning edges with higher dissimilarity, until we are left with a **minimum spanning tree** - that is, a tree with $n$ nodes and $n - 1$ edges. Any further pruning would create **subgraphs**, which become our **cluster candidates**.

### HX8.3.1 Converting to `SpatialPolygonsDataFrame`

The `skater()` function only supports `sp` objects, such as the `SpatialPolygonsDataFrame`. Hence, we need to convert `shan_sf` as needed first.

```{r}
shan_sp <- as_Spatial(shan_sf)
```

### HX8.3.2 Computing Neighbour List

Next, `poly2nd()` from the `spdep` package will be used to compute the neighbours list from the polygon list.

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

We can plot the neighbour list on `shan_sp` by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map.

The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original `SpatialPolygonDataFrame` (*Shan* state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We can also specify `add=TRUE` to plot the network on top of the boundaries.

```{r}
coords <- st_coordinates(
  st_centroid(st_geometry(shan_sf)))
plot(st_geometry(shan_sf), 
     border=grey(.5))
plot(shan.nb,
     coords, 
     col="red", 
     add=TRUE)
```
> Note that if you plot the network first and then the boundaries, some areas will be clipped because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.

## HX8.4 Creating connectivity graph

### HX8.4.1 Calculating edge costs

`nbcosts()` of `spdep` is used to compute the cost of each edge.

```{r}
# From exercise 7
shan_ict <- read_rds('data/rds/shan_ict.rds')
lcosts <- nbcosts(shan.nb, shan_ict)
```

For each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.

Following that, we will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights. In order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below. Note that we have to specify the style as `B` to make sure the cost values are not row-standardised.

```{r}
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B")
summary(shan.w)
```

## HX8.5 Computing minimum spanning tree

It is now time to create our minimum spanning tree. The tree is computed using the `mstree()` method of the `spdep` package.

```{r}
shan.mst <- mstree(shan.w)
# Check class of shan.mst
class(shan.mst)
# Check dimensions
dim(shan.mst)
```

Note that the dimension is **54** and not **55**. This is because the tree comprises $n-1$ *edges* (links) in order to traverse all the nodes.

We can display the content of shan.mst by using head() as shown in the code chunk below.

```{r}
head(shan.mst)
```

We can also plot our tree to show the observation numbers of each node in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.

```{r}
plot(st_geometry(shan_sf), 
                 border=gray(.5))
plot.mst(shan.mst, 
         coords, 
         col="orange2", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```
## HX8.6 Computing spatially constrained clusters using SKATER method

We can now use the code chunk below to compute the spatially constrained clusters using `skater()` from `spdep`. `skater()` takes three mandatory arguments: the first two columns of the MST matrix (i.e. not the cost); the data matrix (to update the costs as units are being grouped), and; the number of cuts. The result of the skater() is an object of class skater, whose contents we can examine.

> Note: The number of cuts is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.

```{r}
clust6 <- spdep::skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
str(clust6)
```

The most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitrary). This is followed by a detailed summary for each of the clusters in the `edges.groups` list. The sum of squares measures are given as `ssto` for the total and `ssw` to show the effect of each of the cuts on the overall criterion.

Given our clusters, we can print the cluster assignment, or use the `table()` function to find out the number of observations in each cluster. Parenthetially, we can also interpret the latter as the dimension of each vector in the lists contained in `edges.groups`. For example, the first list has node with dimension `12`, corresponding to the number of observations in the first cluster.

```{r}
ccs6 <- clust6$groups
ccs6
table(ccs6)
```

Lastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.

```{r}
plot(st_geometry(shan_sf), 
     border=gray(.5))
plot(clust6, 
     coords, 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "yellow4", "purple"),
     cex.circles=0.005, 
     add=TRUE)
```

## HX8.7 Visualising clusters on choropleth map

We can now plot our newly derived clusters onto a map. For easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.

```{r}
groups_mat <- as.matrix(clust6$groups)
# From exercise 7
shan_sf_cluster <- read_rds('data/rds/shan_sf_cluster.rds')
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)

shclust.map <- qtm(shan_sf_spatialcluster, "SP_CLUSTER")
# From exercise 7
hclust.map <- read_rds('data/rds/hmap.rds')
tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```
There is significantly less fragmentation in our SKATER-derived clusters map, and one would notice how the clusters in the map correspond with the graph from earlier.

## HX8.8 Spatially Constrained Clustering: ClustGeo method

### HX8.8.1 A short note about ClustGeo

As described in the course website (Kam, 2024):

> ClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called `hclustgeo()` including spatial/geographical constraints.

> In the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.

> The idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called `choicealpha()`.

### HX8.8.2 Ward-like hierarchical clustering with ClustGeo

The `ClustGeo` package provides us with a function called `hclustgeo()` to perform Ward-like hierarchical clustering. To use this method to perform non-spatially-constrained hierarchical clustering, all we need to provide is a dissimilarity matrix - specifically, the one from Hands-on exercise 7.

```{r}
# From hands-on exercise 7
proxmat <- read_rds('data/rds/proxmat.rds')
nongeo_cluster <- hclustgeo(proxmat)
plot(nongeo_cluster, cex = 0.5)
rect.hclust(nongeo_cluster, 
            k = 6, 
            border = 2:5)
```

### HX8.8.3 Mapping the clusters formed

The ClustGeo clusters can also be plotted onto a map as follows.

```{r}
groups <- as.factor(cutree(nongeo_cluster, k=6))
shan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
qtm(shan_sf_ngeo_cluster, "CLUSTER")
```

## HX8.9 Spatially-constrained hierarchical clustering

Before performing spatially constrained hierarchical clustering, we need to first construct a spatial distance matrix using `st_distance()` from the `sf` package. Notice that `as.dist()` is used to convert the data frame into matrix.

```{r}
dist <- st_distance(shan_sf, shan_sf)
distmat <- as.dist(dist)
```

Next, `choicealpha()` is used to determine a suitable value for the mixing parameter $\alpha$.

```{r}
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)
```
We will choose $\alpha = 0.2$ as per the graphs above.

```{r}
clustG <- hclustgeo(proxmat, distmat, alpha = 0.2)
```

Next, `cutree()` is used to derive the cluster object.
```{r}
groups <- as.factor(cutree(clustG, k=6))
```

Finally, we `cbind` the group list back to `shan_sf`, so that we can plot our newly delineated spatially constrained clusters.

```{r}
shan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
qtm(shan_sf_Gcluster, "CLUSTER")
```

## HX8.10 Visual interpretation of clusters

### HX8.10.1 Visualising individual clustering variable

We can use `ggplot` to plot the distribution of any clustering variable by cluster in a boxplot. In this example, we will plot the distribution of `RADIO_PR`.

```{r}
ggplot(data = shan_sf_ngeo_cluster,
       aes(x = CLUSTER, y = RADIO_PR)) +
  geom_boxplot()
```

The boxplot reveals cluster 3 displays the highest radio penetration rate on average, followed by clusters 2, 1, 4, 6 and 5.

### HX8.10.2 Multivariate visualisation

We can also choose to create multiple parallel coordinate plots, which have been shown to reveal clustering variables extremely effectively. In the below code chunk, parallel coordinate plots can be created by means of `ggparcoord()` from the `GGally` package.

```{r}
ggparcoord(data = shan_sf_ngeo_cluster, 
           columns = c(17:21), 
           scale = "globalminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Multiple Parallel Coordinates Plots of ICT Variables by Cluster") +
  facet_grid(~ CLUSTER) + 
  theme(axis.text.x = element_text(angle = 30))
```

The parallel coordinate plot above reveals that households in Cluster 4 townships have a higher rate of ICT device ownership, while those in cluster 5 have the lowest.

On the possible `scale` arguments for `ggparcoor()`, the course website notes:

> Note that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:

> * std: univariately, subtract mean and divide by standard deviation.
> * robust: univariately, subtract median and divide by median absolute deviation.
> * uniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.
> * globalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.
> * center: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.
> * centerObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param
> * There is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.

### HX8.10.3 Printing summary statistics

The following code chunk creates a summarised table for us to print summary statistics per cluster.

```{r}
shan_sf_ngeo_cluster %>% 
  st_set_geometry(NULL) %>%
  group_by(CLUSTER) %>%
  summarise(mean_RADIO_PR = mean(RADIO_PR),
            mean_TV_PR = mean(TV_PR),
            mean_LLPHONE_PR = mean(LLPHONE_PR),
            mean_MPHONE_PR = mean(MPHONE_PR),
            mean_COMPUTER_PR = mean(COMPUTER_PR))
```

## References

1.  Kam, T. S. (2024). 12 Geographical Segmentation with Spatially Constrained Clustering Techniques. R for Geospatial Data Science and Analytics. <https://r4gdsa.netlify.app/chap12#correlation-analysis>